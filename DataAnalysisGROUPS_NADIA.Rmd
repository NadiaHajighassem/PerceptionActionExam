---
title: "Data analysis"
author: "Nadia and Katharina"
output:
  html_document:
      toc: yes
      number_sections: no
      toc_float: yes
      theme: united
      highlight: espresso
      css: '../../varia/standard.css'
geometry: margin=1in
knit: (function(inputFile, encoding) {
  browseURL(
    rmarkdown::render(
      inputFile,
      encoding = encoding,
      output_dir = "html.file",
      output_file = "DataAnalysis.html"))})
---

```{r setup}
## Setup chunk
knitr::opts_chunk$set(echo = TRUE, include = TRUE, message = FALSE, warning = FALSE)


pacman::p_load(
  "XML",
  "tidyverse",
  "fs",
  "assertthat",
  "stringi",
  "dtw",
  "RTransferEntropy",
  "signal",
  "conflicted",
  "Rcpp",
  "future",
  "fastICA",
  "groupICA",
  "dtw",
  "dplyr",
  "plotly",
  "htmlwidgets",
  "hrbrthemes",
  "zoo"
)
conflicts_prefer(dplyr::filter)
## Making sure we are in the right directory
wd <- getwd()
if (basename(wd) != "PerceptionActionExam") {
  setwd("./PerceptionActionExam")
}

                              ##NADIA PATH##
 # data_dir <- path_home() %>% 
 #   path("Documents", "GitHub", "PerceptionActionExam-Clean-up-attempt", "data", "tsvs") ## Nadia's path

                              #KATHARINA PATH##
data_dir <- path_home() %>%
path("Desktop","UNI", "3.semester", "Perception & Action", "PerceptionActionExam", "data", "tsvs") ## Katharina's path


#Aesthetic setup
theme_set(theme_ipsum(base_family = "Times New Roman"))
global_fill_colour <- "#8d5b5a"
aesthetic_palette <- c(
  "#d8aeb5","#c17f8c","#b59592","#9b6f69","#a94f62","#8d5b5a","#684141","#733545","#523438","#48222b","#2f1a1b")
aesthetic_highlight_difference_palette <- c("#d8aeb5","#2f1a1b")
```

## Pre-processing

Loading functions made by Luke for later processing + making sure we are getting the conditions we want.

```{r loading packages}
# The available conditions and their start and end frame indices
# to use the whole file, just set the value for the condition to c(NA, NA)
conditions <- list(
  jointlead = c(NA, NA),
  leadfollow = c(NA, NA)
)

# Calling functions
source("Functions.R")
```

And so, some more boring stuff...This is just getting the labels so we can make sure that the tracked markers match up with what we expected, really, this shouldn't go wrong, but if you don't check this kind of thing you'll end up scratching your head later wondering why everything broke.

```{r}
# load the labels from the XML file

# load the XML file
xmlfile <- xmlParse("PerAct23_LabelList.xml")

# get the labels, which are in the following format:
# <QTM_Label_List_Ver_1.00>
#     <Trajectories>
#         <Trajectory>
#             <Name>A_head_top</Name>
#             <Color R="0" G="147" B="0"/>
#         </Trajectory>
#     </Trajectories>
# </QTM_Label_List_Ver_1.00>

# get the trajectory names
traj_names <- xpathSApply(xmlfile, "//Trajectory/Name", xmlValue)

# get the trajectory colors
traj_colors <- xpathSApply(xmlfile, "//Trajectory/Color", xmlAttrs)

# convert the colors to hex
traj_colors <- rgb(
  as.numeric(traj_colors[1,]),
  as.numeric(traj_colors[2,]),
  as.numeric(traj_colors[3,]),
  alpha = 255,
  maxColorValue = 255
)


# combine the names and colors into a data frame
traj_labels <- data.frame(
  traj_names,
  traj_colors,
  stringsAsFactors = FALSE
)

rm(xmlfile, traj_names, traj_colors) #cleaning up
```

Finally loading in data hehehe (insert elmo meme with fire in the background)

Aditionally, this code is transforming data from a wide format, where each marker's x, y, and z coordinates are in separate columns, to a long format where all the coordinates are in a single column. It then adds more information about each observation (subject, axis, and marker) before pivoting it back to a wide format.

```{r }
## the code lists files in the specified directory that have names ending with ".tsv" and stores the list of file paths in the variable traj_files.
traj_files <- fs::dir_ls(data_dir, regexp = "\\.tsv$")
```

## Loading the data in and giving meaningful names

We make a list that contains all the individual dataframes. This list will be called on and iterated through throughout the next steps. (was called traj_data_list in lukes script. Here it is list_of_dataframes)

```{r}
# Now we can actually load in the data. When we are loading in the data, we are also renaming our list to be the name of the individual groups and conditions 
list_of_dataframes <- list()

for (file_path in traj_files) {
  # Load the data from the file
  traj_data <- process_qtm_tsv(file_path)
  
  # Extract the group number and condition from the filename
  group_number <- sub(".*group([0-9]+).*\\.tsv", "\\1", basename(file_path))
  condition <- traj_data$metadata$condition
  
  # Create a unique identifier for the combination of group number and condition
  group_condition_identifier <- paste0("group", group_number, "_", condition)
  
  # Check if a dataframe with this identifier already exists in the list
  if (group_condition_identifier %in% names(list_of_dataframes)) {
    # If it exists, append the data to the existing dataframe
    list_of_dataframes[[group_condition_identifier]]$data <- rbind(list_of_dataframes[[group_condition_identifier]]$data, traj_data$data)
  } else {
    # If it doesn't exist, create a new dataframe and add it to the list
    list_of_dataframes[[group_condition_identifier]] <- traj_data
  }
}

# Now, list_of_dataframes contains individual lists for each unique combination of group number and condition

rm(group_condition_identifier,group_number)
```

## Adding condition and group columns, so we can have nice individual dataframes later on

```{r}
list_of_dataframes <- lapply(list_of_dataframes, function(x) {
  # Add condition and group information to the data frame
  x$data$condition <- x$metadata$condition
  
  # Set x$data$group to be the list name
  x$data$group <- basename(file_path)
  
  return(x)
})
```

## Cleaning up the list

Making sure the list only contains the data from each group, and not the metadata.

```{r}
# Assuming list_of_dataframes is a list of data frames with both 'data' and 'metadata'
for (i in seq_along(list_of_dataframes)) {
  x <- list_of_dataframes[[i]]
  
  # Check if 'data' and 'metadata' components exist in each list element
  if (!all(c("data", "metadata") %in% names(x))) {
    warning("List element does not have 'data' and/or 'metadata'. Skipping.")
    next
  }
  
  # Extract condition and group information from the file path
  file_path <- names(list_of_dataframes)[i]
  condition <- sub(".*group[0-9]+_([^_\\.]+).*", "\\1", file_path)
  group_number <- sub(".*group([0-9]+).*", "\\1", file_path)
  
  # Add condition and group information to the data frame
  x$data$condition <- condition
  x$data$group <- paste0("group", group_number)
  
  # Overwrite the original list element with the processed data
  list_of_dataframes[[i]] <- x$data
}

# 'list_of_dataframes' now contains all the processed data frames


rm(x, condition, group_number, i) #Cleaning up
```

## Turning condition and group into factors and ensuring all marker names are not the same

The columns "group" and "condition" shall be looked at as factors, and we are also performing quality control to make sure no markers have weird unique names.

```{r}
library(dplyr)
library(stringi)  # For stri_replace_last_regex
library(assertthat)  # For assert_that

# Loop through each index of the list
for (i in seq_along(list_of_dataframes)) {
  # Extract the current data frame directly from the list
  df <- list_of_dataframes[[i]]
  
  # Check if 'condition' and 'group' columns exist
  if (!all(c("condition", "group") %in% names(df))) {
    warning(paste("Data frame at index", i, "does not have 'condition' and/or 'group' columns. Skipping."))
    next
  }
  
  # Add factors for condition and group
  df$condition <- factor(df$condition)
  df$group <- factor(df$group)
  
  # Print the data frame index
  cat("Data frame at index:", i, "\n")
  
  # Print the first few rows of the data frame
  print(head(df))
  
  # Ensure all marker names are the same
  marker_names <- unique(df %>% select(contains("_x")) %>% names() %>% stri_replace_last_regex("_x", ""))
  
  assert_that(
    all(marker_names == traj_labels$traj_names),
    msg = paste("Not all marker names are the same in Data frame at index", i)
  )
  
  # Update the original data frame in the list
  list_of_dataframes[[i]] <- df
  
  cat("\n")
}
rm(df)
```

## Going from wide format to long format

Final step before we can work on the dataframes. Currently, the

```{r}
library(dplyr)
library(tidyr)
library(stringi)  # For string manipulation

# Loop through each index of the list
for (i in seq_along(list_of_dataframes)) {
  # Extract the current data frame directly from the list
  df <- list_of_dataframes[[i]]
  
  # Check if 'condition' and 'group' columns exist
  if (!all(c("condition", "group") %in% names(df))) {
    warning(paste("Data frame at index", i, "does not have 'condition' and/or 'group' columns. Skipping."))
    next
  }
  
  # Pivot the data
  df <- df %>% 
    pivot_longer(
      cols = contains("_x") | contains("_y") | contains("_z"),
      names_to = "marker",
      values_to = "value"
    ) %>%
    mutate(
      subject = stri_replace_first_regex(marker, "^([AB])_.*", "$1"),
      axis = stri_extract_last_regex(marker, "[xyz]$"),
      marker = stri_replace_first_regex(marker, "^[AB]_([a-zA-Z_]+)_[xyz]$", "$1")
    ) %>%
    # Move axes to columns
    pivot_wider(
      names_from = axis,
      values_from = value
    )
  
  # Update the original data frame in the list
  list_of_dataframes[[i]] <- df
  
  # Print the data frame index
  cat("Data frame at index:", i, "\n")
  
  # Print the first few rows of the pivoted data frame
  print(head(df))
  
  cat("\n")
}
# Now 'list_of_dataframes' contains all the modified data frames
rm(df, i, marker_names)
```

## Sorting by time

```{r}
# Loop through each dataframe in the list and arrange by 'elapsed_time'
list_of_dataframes <- lapply(list_of_dataframes, function(df) {
  df %>% arrange(elapsed_time)
})
```

## Saving CSVs

Now we have succesfully converted the files from Qualysis into dataframes that we can work on throughout the project, we will save each dataframe to a seperate folder. We also clean the environment of any values and dataframes that will not be useful going forward.

```{r}
# Load necessary library
library(tidyverse)

# Check if the list is named; if not, you might need to assign names
if (is.null(names(list_of_dataframes))) {
  warning("The list_of_dataframes is not named. Data frames will be saved with index-based names.")
  names(list_of_dataframes) <- paste0("dataframe_", seq_along(list_of_dataframes))
}

# Create a sub-directory for the CSV files if it doesn't already exist
dir.create("data/csv", showWarnings = FALSE, recursive = TRUE)

# Loop through each data frame in the list
for (df_name in names(list_of_dataframes)) {
  # Define the file path and name for each CSV within the data/csv_files directory
  file_path <- paste0("data/csv/", df_name, ".csv")

  # Save the data frame to a CSV file
  write_csv(list_of_dataframes[[df_name]], file_path)
}

# Inform the user that the operation is complete
cat("All data frames have been saved in the 'data/csv_files' directory with their respective names.\n")


```

A final clean before we work with our CSV files

```{r}
#CLEANING YAY
rm(conditions, traj_data,traj_labels, i, marker_names, traj_files, data_dir, df_name, file_path, traj_files)
```

# Pre-processing: Trimming to 30 seconds, NA's, choosing markers.

We are only interested in a short frame of time within all the recorded data. However, since we will run into issues if we trim before gapfilling, we will save into two seperate dataframes: one that is trimmed into 30 seconds but not gapfilled, and one that is gapfilled before trimming to 30 seconds. We can then use the non-gapfilled version to check for NAs, only within the timeframe we are actually interested in.

```{r copying_data_to_second_df}
list_of_dataframes_unfilled <- list_of_dataframes
```

## Gap-filling

We now commence with gap filling one of the dataframes. Prior to doing this, we ran all the chunks in the NA-checking section to identify the most fitting markers; in order to save computational power, this section is now placed in the front to only visualise the relevant 30 seconds. For a detailed walkthrough of how we decided which markers to use, see following sections.

The reason why we gap-fill before trimming, is in order to account for all the NA's that might exist in the intervals of trimming. This could result in sequences of NA's towards the end and the beginning of the relevant sequence. When writing this document roughly 6000 NAs were eliminated when changing the order of gap-filling and trimming.

In order to later showcase the amount of NA's in the other markers and to validify our choice of markers, the unfilled dataframe is also not filtered, and keeps data for every marker.

```{r markers_of_interest}
# get the markers of interest
markers_of_interest <- c(
  "hand_right",
  "head_left",
  "hand_left",
  "chest"
)

# Apply the filter to each dataframe in list_of_dataframes
list_of_dataframes <- lapply(list_of_dataframes, function(df) {
  df %>% dplyr::filter(marker %in% markers_of_interest)
})
```

One of the lists is now used to gap-fill while the other stays empty:

```{r gap_filling}
# Assuming markers_of_interest is defined and has at least 4 elements
# Assuming gap_fill_linear is a function that performs linear gap filling

for (i in seq_along(list_of_dataframes)) {
  # Access the dataframe
  df <- list_of_dataframes[[i]]

  # Extract a representative 'group' and 'condition' value (assuming they are consistent within each dataframe)
  group_value <- unique(df$group)[1]
  condition_value <- unique(df$condition)[1]

  # Iterate over selected markers
  for (sel_idx in 1:4) {
    # Plot before gap-filling
    plot_before <- df %>% 
      dplyr::filter(marker %in% markers_of_interest, marker == markers_of_interest[sel_idx]) %>%
      ggplot(aes(x = elapsed_time, y = x, color = subject)) +
      geom_line() +
      facet_wrap(~condition) +
      scale_color_manual(values= aesthetic_highlight_difference_palette)+
      labs(
        x = "Elapsed time",
        y = "Marker X position",
        title = paste("Marker", markers_of_interest[sel_idx], "X position before gap filling"),
        subtitle = paste("Group:", group_value, " | ", "Condition:", condition_value)
      )
    print(plot_before)

    # Apply the linear gap fill function to each column, by condition
    df <- df %>% 
      dplyr::group_by(condition, subject, marker) %>%
      dplyr::mutate(across(c(x, y, z), ~ gap_fill_linear(.)))

    # Update the dataframe in the list
    list_of_dataframes[[i]] <- df

    # Plot after gap-filling
    plot_after <- df %>% 
      dplyr::filter(marker %in% markers_of_interest, marker == markers_of_interest[sel_idx]) %>%
      ggplot(aes(x = elapsed_time, y = x, color = subject)) +
      geom_line() +
      facet_wrap(~condition) +
      scale_color_manual(values= aesthetic_highlight_difference_palette)+
      labs(
        x = "Elapsed time",
        y = "Marker X position",
        title = paste("Marker", markers_of_interest[sel_idx], "X position after gap filling"),
        subtitle = paste("Group:", group_value, " | ", "Condition:", condition_value)
      )
    print(plot_after)
    # Save the plot if needed
    # ggsave(...) - add the appropriate ggsave call here if needed
  }
}

# At this point, list_of_dataframes is updated with the gap-filled data

# Combine all dataframes into a single dataframe
combined_df <- dplyr::bind_rows(list_of_dataframes, .id = "group")

rm(plot_after, plot_before, df, sel_idx, i, condition_value, group_value)
```

## Trimming

Before proceeding we must trim the trajectories to accurately match the window of data collection - meaning, we must identify the 30 second window from which the t-pose starts.

Our original dataframes have a frame every 0,003rd second, so in order to save computational power, we simplify the dataframe for visualisation. In this case we will have points every half second. In doing so we can visually identify the point of t-pose start and end.

When we trim both frames, we effectively create a prior and posterior version of our data, which will later be used for NA-checking, euclidian distance, etc.

```{r identifying_t_pose_prep}
# Create a new list with processed dataframes without overwriting the original list
every_half_second_df_list <- lapply(list_of_dataframes, function(df) {
  df %>%
    group_by(subject, marker) %>%
    slice(seq(1, n(), by = 151)) %>%
    ungroup()
})

```

Now, every_half_second_df_list contains the simplified versions of the dataframes: the version with data every half second. Using this list, we can now make 3d plots with an included timeframe and identify the t-poses and effectively our 30 second window.

```{r}
# Create an empty list to store the plots
plot_list <- list()

# Iterate through the simplified dataframes
for (i in seq_along(every_half_second_df_list)) {
  df_name <- names(every_half_second_df_list)[i]
  df <- every_half_second_df_list[[i]]
  
  # Create the base plot
  fig <- plotly::plot_ly(df, 
                 x = ~x, 
                 y = ~y, 
                 z = ~z, 
                 type = "scatter3d", 
                 mode = "markers", 
                 size = 2,
                 frame = ~elapsed_time,
                 marker = list(size = 4), ## Adjusting the marker size
                 color = ~subject)
  
  fig <- fig %>% plotly::layout(
    scene = list(
      xaxis = list(title = "X-axis", range = c(-800, 800)), 
      yaxis = list(title = "Y-axis", range = c(-800, 800)),
      zaxis = list(title = "Z-axis", range = c(0, 1800)),
      aspectmode = "manual",
      aspectratio = list(x = 1, y = 1, z = 1)
    ),
    xaxis = list(title = "X-axis"), 
    yaxis = list(title = "Y-axis"),
    zaxis = list(title = "Z-axis"))
  
  fig <- fig %>% ## adding and changing text
    plotly::layout(title = list(text = df_name, y = 0.9), 
                   font=list(size=15, family = "Times new roman"),
                   legend = list(title = list(text = "markers")))
  
  plot_list[[i]] <- fig
}

# Now plot_list contains all the plots for each simplified dataframe

# Save each plot in plot_list as an HTML file
for (i in seq_along(plot_list)) {
  plot <- plot_list[[i]]
  filename <- paste0("plot_", i, ".html")
  saveWidget(plot, file = filename)
}
```

```{r}
plot_list
rm(df, fig, plot, i, df_name, filename, current_df)
```

Having manually identified the relevant time-frames, we create a new dataframe and input the relevant values of elapsed time.

```{r}
start_time_df <- data.frame(
  group_and_condition = c("group0_jointlead",
                          "group0_leadfollow",
                          "group1_jointlead",
                          "group1_leadfollow",
                          "group12_jointlead",
                          "group12_leadfollow",
                          "group13_jointlead",
                          "group13_leadfollow",
                          "group2_jointlead",
                          "group2_leadfollow",
                          "group3_jointlead",
                          "group3_leadfollow",
                          "group4_leadfollow",
                          "group6_jointlead",
                          "group6_leadfollow",
                          "group8_leadfollow"),
  elapsed_time_at_start = as.numeric(c("17",
                            "35",
                            "12",
                            "50",
                            "33",
                            "28",
                            "37",
                            "94",
                            "18",
                            "31",
                            "12",
                            "42",
                            "37",
                            "14",
                            "29",
                            "52")
                            )
)
#We no longer need the simplified dataframe :)
rm(every_half_second_df_list)
```

Now we will iterate through all the group dataframes and pick the elapsed_time value associated. First we do this for the gap-filled list:

```{r trimming_gap_filled_list}
# Iterate through rows of start_time_df and extract subsets
for (i in 1:nrow(start_time_df)) {
  df_name <- start_time_df$group_and_condition[i]
  start_time <- start_time_df$elapsed_time_at_start[i]
  
  # Find the corresponding dataframe in list_of_dataframes, if it exists
  if (df_name %in% names(list_of_dataframes)) {
    # Access the dataframe directly from the list
    df <- list_of_dataframes[[df_name]]
    
    # Arrange the dataframe by elapsed_time
    df <- df %>% dplyr::arrange(elapsed_time)
    
    # Extract the subset based on the starting value
    starting_index <- which(df$elapsed_time == start_time)[1]
    ending_index <- which(df$elapsed_time == start_time + 30)[1]
    
    if (!is.na(starting_index) && !is.na(ending_index)) {
      # Update the original dataframe in the list with the subset
      list_of_dataframes[[df_name]] <- df[starting_index:ending_index, ]
    }
    # No else block needed, if indices are not found, the original dataframe remains unchanged
  }
  # No else block needed, if the dataframe does not exist in the list, nothing happens
}

# Now, list_of_dataframes contains the updated dataframes with the subsets

```

Next we will trim the unfilled list:

```{r trimming_unfilled_list}
# Iterate through rows of start_time_df and extract subsets
for (i in 1:nrow(start_time_df)) {
  df_name <- start_time_df$group_and_condition[i]
  start_time <- start_time_df$elapsed_time_at_start[i]
  
  # Find the corresponding dataframe in list_of_dataframes, if it exists
  if (df_name %in% names(list_of_dataframes_unfilled)) {
    # Access the dataframe directly from the list
    df <- list_of_dataframes_unfilled[[df_name]]
    
    # Arrange the dataframe by elapsed_time
    df <- df %>% dplyr::arrange(elapsed_time)
    
    # Extract the subset based on the starting value
    starting_index <- which(df$elapsed_time == start_time)[1]
    ending_index <- which(df$elapsed_time == start_time + 30)[1]
    
    if (!is.na(starting_index) && !is.na(ending_index)) {
      # Update the original dataframe in the list with the subset
      list_of_dataframes_unfilled[[df_name]] <- df[starting_index:ending_index, ]
    }
    # No else block needed, if indices are not found, the original dataframe remains unchanged
  }
  # No else block needed, if the dataframe does not exist in the list, nothing happens
}

# Now, list_of_dataframes contains the updated dataframes with the subsets

#cleaning up again! 
rm(df, df_name, ending_index, first_index, i, start_time, starting_index, start_time_df)
```

## Visualising gap-filling on trimmed data

First we visualise the untrimmed dataset:

```{r}
# Assuming markers_of_interest is defined and has at least 4 elements
# Assuming gap_fill_linear is a function that performs linear gap filling

for (i in seq_along(list_of_dataframes_unfilled)) {
  # Access the dataframe
  df <- list_of_dataframes_unfilled[[i]]

  # Create a combined plot for each group and condition, faceted by marker
  unique_groups <- unique(df$group)
  unique_conditions <- unique(df$condition)

  for (group in unique_groups) {
    for (condition in unique_conditions) {
      plot_combined <- df %>% 
        dplyr::filter(group == group, condition == condition, marker %in% markers_of_interest) %>%
        ggplot(aes(x = elapsed_time, y = x, color = subject)) +
        geom_line() +
        scale_color_manual(values= aesthetic_highlight_difference_palette)+
        facet_wrap(~marker, nrow = 2, ncol = 2) +  # Arrange facets in a 2x2 grid by marker
        labs(
          x = "Elapsed time",
          y = "Marker X position",
          title = paste("Before Gap-filling"),
          subtitle = paste("Group:", group, "| Condition:", condition, "- Marker X Positions")
        )
      print(plot_combined)
    }
  }
}
```

There are a couple of errors clearly visible in the plots, that will have an impact on our analysis. For instance, group 8 gets cut off too early. Meaning the recording was either stopped before it should have been, or has been cut short during the session afterwards.

Then we visualise the data after gap-filling:

```{r}
# Assuming markers_of_interest is defined and has at least 4 elements
# Assuming gap_fill_linear is a function that performs linear gap filling

for (i in seq_along(list_of_dataframes)) {
  # Access the dataframe
  df <- list_of_dataframes[[i]]

  # Create a combined plot for each group and condition, faceted by marker
  unique_groups <- unique(df$group)
  unique_conditions <- unique(df$condition)

  for (group in unique_groups) {
    for (condition in unique_conditions) {
      plot_combined <- df %>% 
        dplyr::filter(group == group, condition == condition, marker %in% markers_of_interest) %>%
        ggplot(aes(x = elapsed_time, y = x, color = subject)) +
        geom_line() +
        scale_color_manual(values= aesthetic_highlight_difference_palette)+
        facet_wrap(~marker, nrow = 2, ncol = 2) +  # Arrange facets in a 2x2 grid by marker
        labs(
          x = "Elapsed time",
          y = "Marker X position",
          title = paste("After Gap-filling"),
          subtitle = paste("Group:", group, "| Condition:", condition, "- Marker X Positions")
        )
      print(plot_combined)
    }
  }
}
```

## NA checking pre-gapfill

As clarified prior to gap filling, the following sections were used to choose which markers to include in our analysis. In the following we will use the list of dataframes that have been cut to the relevant 30 seconds yet not gap-filled to give a thorough overview of the actually relevant amount of NAs.

For the most accurate analysis we wanted to include the following markers: Head, both hands and elbows, and chest. We checked for NA's to verify none of these markers have too many NAs.

```{r}
combined_df <- dplyr::bind_rows(list_of_dataframes_unfilled) %>% 
  dplyr::ungroup()  

combined_df %>% 
  dplyr::summarise(Total_NAs = sum(is.na(x)))
combined_df %>% 
  dplyr::summarise(Total_NON_NAs = sum(!is.na(x)))

#NA
na_pre_gap <- combined_df %>%
  group_by(marker) %>%
  summarise(na_count = sum(is.na(x))) %>% 
  arrange(na_count)

#NON NA
non_na_pre_gap <-combined_df %>%
  group_by(marker) %>%
  summarise(non_na_count = sum(!is.na(x))) %>% 
  arrange(non_na_count)

print(na_pre_gap)
print(non_na_pre_gap)

rm(non_na_pre_gap, na_pre_gap)
```

### Identifying who has an extra headmarker

We noticed that head_top didn't have a lot of NA's, so we checked whether or not it was the extra marker for A

```{r checking to see which markers B have}
  list_of_dataframes_unfilled$group0_jointlead %>%
  dplyr::filter(subject == "B") %>%
  distinct(marker)

## Subject B has no head_top heheheheheee
```

head_top does not appear as a marker in the list, given subject B was only given 2 dots on their head and A was given two; therefore, head_top is the "missing" marker and we therefore cannot use it to compare subjects.

## Visualising the NAs

```{r}
# Combine all dataframes in list_of_dataframes into one dataframe
combined_df <- dplyr::bind_rows(list_of_dataframes_unfilled, .id = "group")

# Identifying columns with NAs and processing
na_columns <- colnames(combined_df)[colSums(is.na(combined_df)) > 0]
columns_to_summarize <- c("marker", na_columns, "group", "condition")

# Summarize the number of NAs for each marker
result <- combined_df %>%
  dplyr::select(dplyr::all_of(columns_to_summarize)) %>%
  dplyr::group_by(group, marker, condition) %>%
  dplyr::summarise(across(everything(), ~ sum(is.na(.))), .groups = "drop") %>%
  dplyr::arrange(marker, condition)

# Plotting with facet wrap
p <- ggplot(result, aes(x = marker, y = x, fill = marker)) +
  geom_bar(stat = "identity", position = "dodge", show.legend = FALSE) +
  labs(title = "NA's in Combined Dataframes", x = "Marker", y = "Number of NA") +
  facet_wrap(~ condition) +
  coord_flip()+
  scale_fill_manual(values = aesthetic_palette)

# Print the combined plot
print(p)
rm(p, result, columns_to_summarize, na_columns)
```

```{r}
# Now we can get the longest sequence of NAs for each marker
# Function to calculate the longest NA run length
max_na_run_length <- function(vec) {
  rle_na <- rle(is.na(vec))
  max(rle_na$lengths[rle_na$values], na.rm = TRUE, default = 0)
}

# Now we can get the longest sequence of NAs for each marker
longest_na_seq <- combined_df %>% 
  group_by(condition, marker) %>%
  summarise(
    x_length = max_na_run_length(x),
    .groups = "drop"
  )

# plot to check if it is acceptable
longest_na_seq %>% 
  ggplot(aes(x = marker, y = x_length, fill=marker)) +
  geom_col(
    show.legend = FALSE
  ) +
  coord_flip() +
  facet_wrap(c(~condition)) +
  labs(
    x = "Marker",
    y = "Longest NA sequence",
    title = "Longest NA sequence by condition"
  )+
  scale_fill_manual(values = aesthetic_palette)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Unfortunately, the elbow markers have a lot of NA's. According to our plots, the head markers are consistently low in NA's, while the distribution of NA's for hands and chest is not consistent throughout the groups, we continue with checking the euclydian distance to further verify the validity of choosing said markers. However, the head_top is only present for participant A, and therefore we must choose either head_left or head_right. Since head_right has fewer NA's and the longest sequence is also smaller, we proceed with head_right

### Euclidean distance

```{r Euclidean distance plots for each marker}
# Iterate directly over the dataframes in list_of_dataframes
for (df in list_of_dataframes_unfilled) {
  # Calculate the euclidean distance between each marker (using x, y, z)
  # We will do this by subject, marker, and axis
  marker_distances <- df %>% 
    dplyr::group_by(subject, marker) %>%
    dplyr::arrange(index) %>%
    dplyr::mutate(
      diff_x = x - dplyr::lag(x, 1),
      diff_y = y - dplyr::lag(y, 1),
      diff_z = z - dplyr::lag(z, 1)
    ) %>%
    dplyr::mutate(
      euclidean_distance = sqrt(diff_x^2 + diff_y^2 + diff_z^2)
    )

  # Plot the series for each marker, and see if anything stands out
  plot <- ggplot(marker_distances, aes(x = index, y = euclidean_distance)) +
    geom_line(aes(color = factor(marker)), linewidth = 1.25) +
    theme_minimal() +
    facet_wrap(~condition + subject) +
    labs(
      x = "Index",
      y = "Euclidean distance",
      title = "Euclidean distance from the previous frame by marker",
      subtitle = paste( df$group[1], "Condition:", df$condition[1])
    )+
  scale_color_manual(values = aesthetic_palette)
  
  # Print the plot
  print(plot)
}
rm(df, df_name, marker_distances, plot)
```

```{r euclidean_distance_selected_markers}
# Assuming list_of_dataframes is a list of dataframes

for (df in list_of_dataframes_unfilled) {
  # Calculate the euclidean distance between each marker (using x, y, z)
  # We will do this by subject, marker, and axis
  marker_distances <- df %>% 
    dplyr::group_by(subject, marker) %>%
    dplyr::arrange(index) %>%
    dplyr::mutate(
      diff_x = x - dplyr::lag(x, 1),
      diff_y = y - dplyr::lag(y, 1),
      diff_z = z - dplyr::lag(z, 1)
    ) %>%
    dplyr::mutate(
      euclidean_distance = sqrt(diff_x^2 + diff_y^2 + diff_z^2)
    ) %>%
    # Filter only the specified markers
    dplyr::filter(marker %in% c("head_right", "hand_right", "hand_left", "chest"))

  # Plot the series for each marker, and see if anything stands out
  plot <- ggplot(marker_distances, aes(x = index, y = euclidean_distance)) +
    geom_line(aes(color = factor(marker)), linewidth = 1.25) +
    theme_minimal() +
    facet_wrap(~condition + subject) +
    labs(
      x = "Index",
      y = "Euclidean distance",
      title = "Euclidean distance from the previous frame by marker",
      subtitle = paste("Group:", df$group[1], "Condition:", df$condition[1])
    )+
  scale_color_manual(values = c("#d8aeb5","#a94f62","#8d5b5a","#2f1a1b"))
  
  # Print the plot
  print(plot)
}
rm(df, marker_distances, plot)
```

### Inspecting the raw data by plotting the x-coordinates

(inspecting to further validate the selection of markers -\> the markers chosen account for most of movement)

```{r for loop that plots x-coordinates for all markers in all groups}
## Making a for loop that plots x-coordinates for all markers in all groups
for (df in list_of_dataframes_unfilled) {
 # Create ggplot
  plot <- ggplot(df, aes(x = x, y = marker, color = global_fill_colour)) +
    geom_point() +
    ggtitle(paste("Group:", df$group[1], "Condition:", df$condition[1]))+
    scale_color_manual(values = global_fill_colour)+
    theme(legend.position = "none") 
  
  # Print the plot
  print(plot)
}
rm(df, plot)
```

It is clear the quality of the dataset differs immensely, and some groups have significantly more NA's than others.

## NA-check only relevant markers pre gap-fill

```{r}
# Apply the filter to each dataframe in list_of_dataframes
list_of_dataframes_unfilled <- lapply(list_of_dataframes_unfilled, function(df) {
  df %>% dplyr::filter(marker %in% markers_of_interest)
})
#overwriting the combined
combined_df <- dplyr::bind_rows(list_of_dataframes_unfilled) %>% 
  dplyr::ungroup()  

#Totals
combined_df %>% 
  dplyr::summarise(Total_NAs = sum(is.na(x)))

combined_df %>% 
  dplyr::summarise(Total_NON_NAs = sum(!is.na(x)))
#NA

na_post_gap <- combined_df %>%
  group_by(marker) %>%
  summarise(na_count = sum(is.na(x)))

#NON NA
non_na_post_gap <- combined_df %>%
  group_by(marker) %>%
  summarise(non_na_count = sum(!is.na(x)))

print(non_na_post_gap)
print(na_post_gap)

rm(non_na_post_gap, na_post_gap)
```

## NA check post gap-fill

```{r}
#overwriting the combined with the gap filled from the list
combined_df <- dplyr::bind_rows(list_of_dataframes) %>% 
  dplyr::ungroup()  

#Totals
combined_df %>% 
  dplyr::summarise(Total_NAs = sum(is.na(x)))

combined_df %>% 
  dplyr::summarise(Total_NON_NAs = sum(!is.na(x)))
#NA

na_post_gap <- combined_df %>%
  group_by(marker) %>%
  summarise(na_count = sum(is.na(x)))

#NON NA
non_na_post_gap <- combined_df %>%
  group_by(marker) %>%
  summarise(non_na_count = sum(!is.na(x)))

print(non_na_post_gap)
print(na_post_gap)

rm(non_na_post_gap, na_post_gap)
```

We still have a lot of NA's caused by inadequent registering of markers during experiment. After gap-filling the total NAs have gone from 169725 to 93280.

\-\-\-\-\-\--

# ICA: Independent Component Analysis

## ICA-preprocessing

In order for ICA to work, our data must be exclusively numeric, meaning we will need to prep dataframes to include only our coordinates. The goal is to get a measure of the subjects movement per time, therefore we split our data into two: A and B. We also omit any NAs. This is subpar, but we have decided it is better to omit NAs entirely rather than apply an average throughout the missing rows - as this might skew it in favour of looking more synchronised.

```{r}
list_of_dataframes_A <- list()
list_of_dataframes_B <- list()

# Get the names of the original dataframes
original_names <- names(list_of_dataframes)

for (i in seq_along(list_of_dataframes)) {
    df <- list_of_dataframes[[i]]
    df_name <- original_names[i]

    # Split the dataframe based on 'subject' and omit rows with NAs
    df_A <- na.omit(df[df$subject == 'A', ])
    df_B <- na.omit(df[df$subject == 'B', ])

    # Add the split and cleaned dataframes to their respective lists with modified names
    if(nrow(df_A) > 0) {
        list_of_dataframes_A[[paste(df_name, "A", sep = "_")]] <- df_A
    }
    if(nrow(df_B) > 0) {
        list_of_dataframes_B[[paste(df_name, "B", sep = "_")]] <- df_B
    }
}
rm(df, df_A, df_B, i, df_name)
```

Now that we have split our data, we need to convert it into a matrix with xyz coordinates where each row is a time-frame. In order to do this, we need to go back to a wider format, where each row is a single time point. This means each marker x y z will take up columns. We exclude elapsed_time, because it will be represented row-by-row. We also need to ungroup the dataframe, otherwise we can't remove columns. Furthermore, once changed to matrix, we have no choice but to impute NA or discard all rows with NAs, which is not viable.

```{r}
# Define a function to replace NAs with column means
replace_nas_with_column_means <- function(df) {
  df %>%
    mutate(across(everything(), ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))
}

# Iterate over list_of_dataframes_A and replace NAs with column means
for (i in seq_along(list_of_dataframes_A)) {
  list_of_dataframes_A[[i]] <- list_of_dataframes_A[[i]] %>%
    ungroup() %>%
    select(-condition, -index, -subject, -group) %>%
    gather(key, value, -elapsed_time, -marker) %>%
    unite(col = "new_col", marker, key, remove = TRUE) %>%
    spread(key = new_col, value = value) %>%
    arrange(elapsed_time) %>%
    replace_nas_with_column_means()
}

# Iterate over list_of_dataframes_B and replace NAs with column means
for (i in seq_along(list_of_dataframes_B)) {
  list_of_dataframes_B[[i]] <- list_of_dataframes_B[[i]] %>%
    ungroup() %>%
    select(-condition, -index, -subject, -group) %>%
    gather(key, value, -elapsed_time, -marker) %>%
    unite(col = "new_col", marker, key, remove = TRUE) %>%
    spread(key = new_col, value = value) %>%
    arrange(elapsed_time) %>%
    replace_nas_with_column_means()
}
rm(i)
```

Now we are ready to convert to matrix, we also standardize as a step in this function with scale(). ICA often benefit from having its features on the same scale, and therefore standardising before running ICA is good practise.

```{r}
# Function to convert a dataframe to a matrix with standardized values for ICA
convert_df_to_matrix_for_ICA <- function(df) {
    # Ensure the dataframe is sorted by elapsed_time
    df_sorted <- df %>% arrange(elapsed_time)

    # Select only numeric columns
    df_numeric <- df_sorted %>% select(where(is.numeric))

    # Standardize the data
    df_standardized <- as.data.frame(scale(df_numeric))

    # Return as matrix
    return(as.matrix(df_standardized))
}

# Convert each dataframe in list_of_dataframes_A to a matrix for ICA
matrices_A <- lapply(list_of_dataframes_A, convert_df_to_matrix_for_ICA)

# Convert each dataframe in list_of_dataframes_B to a matrix for ICA
matrices_B <- lapply(list_of_dataframes_B, convert_df_to_matrix_for_ICA)
```

```{r}
# Check for NAs in matrices_A
any(is.na(unlist(matrices_A)))

# Check for NAs in matrices_B
any(is.na(unlist(matrices_B)))

```

## Deciding on amount of components

We want to make sure we select the optimal amount of components to describe each subjects motion.

```{r}

```

## Running ICA

Though we may lose some data-complexity, we decide to extract only 1 component. Furthermore, ICA sometimes flips data as it does not inherently know which direction the data faces. This is a big issue when using ICA before dynamtic time warping, as we need to make sure both components A and B have the same direction.

Since we know the components represent synchrony we compute the area between the components in all configurations, and choose the configuration where the area is the smallest. In other words, the components will be flipped in a way to consistently be close to eachother and therefore this should result in them being faced "the same way". This method only works because we know the nature of the components is to attempt to follow eachother for A and B.

We will discuss using ICA with motion capture data as a method in the discussion.

First we apply ICA on the matrix:

```{r}
# Function to apply ICA on a matrix
apply_ica <- function(matrix_data, n_comp) {
  # Assuming the first column is 'elapsed_time' and should not be included in ICA
  matrix_for_ica <- matrix_data[, -1]  # Exclude 'elapsed_time'
  
  # Run ICA (nAdiA:I CHANGED ROW.NORM)
  if(ncol(matrix_for_ica) >= n_comp) {
    ica_result <- fastICA(matrix_for_ica, n.comp = n_comp,alg.typ = "parallel", fun = "logcosh", alpha = 1,
             method = "R", row.norm = TRUE, maxit = 200,
             tol = 0.0001, verbose = TRUE)
    return(ica_result)
  } else {
    warning(paste("Not enough columns in matrix to extract", n_comp, "components"))
    return(NULL)
  }
}

# Apply ICA on matrices in list_of_dataframes_A
ica_results_A <- lapply(matrices_A, apply_ica, n_comp = 1)  # Adjust n.comp as needed


# Apply ICA on matrices in list_of_dataframes_B
ica_results_B <- lapply(matrices_B, apply_ica, n_comp = 1)  # Adjust n.comp as needed

component_list_A <- lapply(ica_results_A, function(ica) ica$S)
component_list_B <- lapply(ica_results_B, function(ica) ica$S)

rm(ica_results_A, ica_results_B)
```

Next we calculate the area between the components and pick the combination which results in the smallest area: in our case that must be the case where they are aligned.

```{r}
# Function to calculate the area between two components, aligning their lengths
calculate_area_between_components <- function(comp1, comp2) {
  # Align lengths
  min_length <- min(nrow(comp1), nrow(comp2))
  comp1_aligned <- comp1[1:min_length, ]
  comp2_aligned <- comp2[1:min_length, ]

  # Calculate area
  sum(abs(comp1_aligned - comp2_aligned))
}

# Function to align a component based on the smallest area between lines
align_component_based_on_area <- function(component_A, component_B) {
  # Align component lengths for area calculation
  original_area <- calculate_area_between_components(component_A, component_B)
  flipped_area <- calculate_area_between_components(component_A, -component_B)

  if (flipped_area < original_area) {
    return(-component_B)
  } else {
    return(component_B)
  }
}

# Align each pair of components from A and B
for (i in seq_along(component_list_A)) {
  if (!is.null(component_list_A[[i]]) && !is.null(component_list_B[[i]])) {
    component_list_B[[i]] <- align_component_based_on_area(component_list_A[[i]], component_list_B[[i]])
  }
}


# Clean up
rm(list_of_dataframes_A, list_of_dataframes_B)

```

Now we visualise the components per condition in a common plot, to make it clear which representations of motion each subject has, effectively making it easier to visualise their pathways as a last step before DTW.

```{r}
# Loop through each index of the component lists
for (i in seq_along(component_list_A)) {
  # Extract components for Subject A and Subject B
  component_A <- component_list_A[[i]]
  component_B <- component_list_B[[i]]

  # Ensure both components have the same number of rows
  min_rows <- min(nrow(component_A), nrow(component_B))

  # Combine both components into a single dataframe
  combined_df <- data.frame(
    time = rep(1:min_rows, 2),
    value = c(component_A[1:min_rows, 1], component_B[1:min_rows, 1]),
    group = rep(c("Subject A", "Subject B"), each = min_rows)
  )

  # Create a time series plot for both subjects
  component_name_A <- names(component_list_A)[i]
  component_name_B <- names(component_list_B)[i]
  group_number <- paste(component_name_A, "vs", component_name_B)

  
  p <- ggplot(combined_df, aes(x = time, y = value, color = group)) +
    geom_line(size = 1) +
    labs(
      title = "Independent components", 
      subtitle = group_number,
         x = "Time", y = "Component Value", color = "Subject") +
    scale_color_manual(values = aesthetic_highlight_difference_palette)

  # Print the plot
  print(p)
}


```

OBS: CHECK GROUP 12 LEADFOLLOW, GROUP 13 JOINTLEAD & LEADFOLLOW, GROUP 6 JOINTLEAD, -\> ARE THE COMPONENTS NOT REFLECTIVE OF WHATS GOING ON OR ARE THEY JUST REALLY BAD AT JOINT MOTION.

# DTW: Dynamic Time Warping

## Scoring each pair

Computing DTW for each group with a for loop, results can be found in "dtw_results_df"

```{r}
# Create empty vectors to store DTW distances, group numbers, and conditions
dtw_distances <- numeric(length(component_list_A))
group_numbers <- numeric(length(component_list_A))
conditions <- character(length(component_list_A))

# Loop through each pair of dataframes
for (i in seq_along(component_list_A)) {
  # Extract group number and condition from the element name for component_list_A
  matches <- regmatches(names(component_list_A)[i], regexec("group([0-9]+)_([^_]+)", names(component_list_A)[i]))
  group_number <- as.numeric(matches[[1]][2])
  condition <- matches[[1]][3]
  
  # Calculate DTW distance for component_list_A
  dtw_result <- dtw::dtw(component_list_A[[i]], component_list_B[[i]], keep = TRUE)
  
  # Extract DTW distance for component_list_A
  dtw_distance <- dtw_result$distance
  
  # Print the DTW distance for component_list_A
  print(paste("Group", group_number, condition, "DTW Distance:", dtw_distance))
  
  # Store DTW distance, group number, and condition in the vectors
  dtw_distances[i] <- dtw_distance
  group_numbers[i] <- group_number
  conditions[i] <- condition
}

# Create a data frame with results
dtw_results_df <- data.frame(Group = group_numbers, Condition = conditions, DTW_Distance = dtw_distances)

# Print overall summary or take further actions with dtw_distances vector
cat("Summary of DTW Distances:\n")
summary(dtw_distances)

# Display the data frame with results
print(dtw_results_df)

```

## Visualising

### Testing step patterns

```{r testing_step_patterns}
i<- 6
plot(
    dtw(component_list_A[[i]],component_list_B[[i]],
        keep=TRUE,
        step=rabinerJuangStepPattern(6,"c")),
        col = aesthetic_highlight_difference_palette,
         lty = c(1, 1),
         lwd = c(6, 6),
        type="twoway",offset=-2)

plot(
    dtw(component_list_A[[i]],component_list_B[[i]],
        keep=TRUE,
        step=symmetricP1),
        col = aesthetic_highlight_difference_palette,
         lty = c(1, 1),
         lwd = c(6, 6),
        type="twoway",offset=-2)
plot(
    dtw(component_list_A[[i]],component_list_B[[i]],
        keep=TRUE,
        step=symmetricP2),
        col = aesthetic_highlight_difference_palette,
         lty = c(1, 1),
         lwd = c(6, 6),
        type="twoway",offset=-2)

plot(
    dtw(component_list_A[[i]],component_list_B[[i]],
        keep=TRUE,
        step=rabinerJuangStepPattern(7,"b")),
        col = aesthetic_highlight_difference_palette,
         lty = c(1, 1),
         lwd = c(6, 6),
        type="twoway",offset=-2)
```

### Visualising all pairs:

```{r twoway}
for (i in seq_along(component_list_A)) {
    # TWO WAY plot
    plot(dtw(component_list_A[[i]],
             component_list_B[[i]],
             keep = TRUE,
             step = rabinerJuangStepPattern(6, "c")),
         type = "twoway",
         lty = c(1, 1),
         lwd = c(6, 6),
         offset = -2,
         col = aesthetic_highlight_difference_palette)

    # THREE WAY plot
    plot(dtw(component_list_A[[i]],
             component_list_B[[i]],
             k = TRUE),
         type = "threeway",
         off = 1,
         match.lty = 2,
         col = aesthetic_highlight_difference_palette)

    # DENSITY plot
    plot(dtw(component_list_A[[i]],
             component_list_B[[i]],
             k = TRUE),
         type = "density",
         off = 1,
         match.lty = 2)
}

```

This is a plot with a specificed step-pattern. I dont think we have to do this for the exam, as i saw Luuk not doing it and the differences between the step pattern seems quite hard to interpret on the internet. But to be fair, it looks pretty awesome, but there are a lot of other step-patterns out there and i am not sure, that this is the correct one to use for our data. Especially because this step pattern looks awesome with this group, but looks terrible with others.

Kat's answer:

I have found 3 different step patterns: Sakoe-chiba Band, Itakura Parallelogram and then Rabiner Juang (the one Luke is using), but it seems like you can define your own custom step pattern as well - that might be the most common? Anyways GPT says "The one you are currently using is typically used for speech recognition. It's good for sequences where one sequence may have more variance in timing or speed than the other."

## False-pairs

Having given each pair a score for their synchrony, we must now explore if this score is significant or not. We will do this by establishing false-pairs, looking at each subject A with all other subject B and scoring them. This will result in a spectrum of synchrony that is hopefully normally distrubuted. We will then see that the score for the correct pair is statistically significant.

We first look at the jointlead condition

```{r}
# Initialize vectors to store results
dtw_distances <- numeric(0)
component_a_names <- character(0)
component_b_names <- character(0)

# Loop through each element of component_list_A
for (a_name in names(component_list_A)) {
    if (grepl("_jointlead_A$", a_name)) {
        component_a <- component_list_A[[a_name]]

        # Loop through each element of component_list_B for every qualifying component in component_list_A
        for (b_name in names(component_list_B)) {
            if (grepl("_jointlead_B$", b_name)) {
                # Perform DTW comparison
                dtw_result <- dtw::dtw(component_a, component_list_B[[b_name]], keep = TRUE)
                
                # Store DTW distance and corresponding A and B component names
                dtw_distances <- c(dtw_distances, dtw_result$distance)
                component_a_names <- c(component_a_names, a_name)
                component_b_names <- c(component_b_names, b_name)
            }
        }
    }
}

# Create a data frame with results if any comparisons were made
if (length(dtw_distances) > 0) {
    dtw_results_df_jointlead <- data.frame(
        Component_A = component_a_names, 
        Component_B = component_b_names, 
        DTW_Distance = dtw_distances
    )
    print(dtw_results_df_jointlead)
} else {
    cat("No matching pairs found for '_jointlead_A' and '_jointlead_B'")
}

```

```{r}
# Initialize vectors to store results
dtw_distances <- numeric(0)
component_a_names <- character(0)
component_b_names <- character(0)

# Loop through each element of component_list_A
for (a_name in names(component_list_A)) {
    if (grepl("_leadfollow_A$", a_name)) {
        component_a <- component_list_A[[a_name]]

        # Loop through each element of component_list_B for every qualifying component in component_list_A
        for (b_name in names(component_list_B)) {
            if (grepl("_leadfollow_B$", b_name)) {
                # Perform DTW comparison
                dtw_result <- dtw::dtw(component_a, component_list_B[[b_name]], keep = TRUE)
                
                # Store DTW distance and corresponding A and B component names
                dtw_distances <- c(dtw_distances, dtw_result$distance)
                component_a_names <- c(component_a_names, a_name)
                component_b_names <- c(component_b_names, b_name)
            }
        }
    }
}

# Create a data frame with results if any comparisons were made
if (length(dtw_distances) > 0) {
    dtw_results_df_leadfollow <- data.frame(
        Component_A = component_a_names, 
        Component_B = component_b_names, 
        DTW_Distance = dtw_distances
    )
    print(dtw_results_df_leadfollow)
} else {
    cat("No matching pairs found for '_leadfollow_A' and '_leadfollow_B'")
}

```

Now joining the two dataframes together

```{r}
# Combine the two dataframes
false_pairs_df <- rbind(dtw_results_df_leadfollow, dtw_results_df_jointlead)

rm(dtw_results_df_jointlead, dtw_results_df_leadfollow)
```

```{r}
# Loop over each unique value in the 'Component A' column
for (comp_a in unique(false_pairs_df$Component_A)) {
    # Filter the dataframe for the current component
    filtered_df <- subset(false_pairs_df, Component_A == comp_a)

    # Create a density plot
    p <- ggplot(filtered_df, aes(x = DTW_Distance)) +
        geom_density(fill = global_fill_colour, alpha = 0.5) +
      labs(
        title = paste("Density Plot of DTW Distances for Component A:"),
        subtitle = comp_a)

    # Display the plot
    print(p)
}

# Loop over each unique value in the 'Component B' column
for (comp_b in unique(false_pairs_df$Component_B)) {
    # Filter the dataframe for the current component
    filtered_df <- subset(false_pairs_df, Component_B == comp_b)

    # Create a density plot
    p <- ggplot(filtered_df, aes(x = DTW_Distance)) +
        geom_density(fill = global_fill_colour, alpha = 0.5) +
      labs(
        title = paste("Density Plot of DTW Distances for Component B:"),
        subtitle = comp_b)

    # Display the plot
    print(p)
}
```

## Real pairs

Taking the mean of DTW for all groups to get a baseline and checking which groups are below the mean and therefore high in similarity.

```{r}
DTW_mean <- mean(dtw_results_df$DTW_Distance) 

print(paste("mean:", DTW_mean))
for (i in seq_along(dtw_results_df$DTW_Distance)) {
  dtw_distance <- dtw_results_df$DTW_Distance[i]
  if (dtw_distance <= DTW_mean) {
    group <- dtw_results_df$Group[i]
    condition <- dtw_results_df$Condition[i]
    print(paste("DTW_Distance:", dtw_distance, "Group:", group, "Condition:", condition))
  }
}
```

```{r}
dtw_results_df %>%
  ggplot(aes(x = DTW_Distance)) +
  geom_density(fill = global_fill_colour, alpha = 0.5) +  # Add fill color and adjust transparency
  labs(
    title = "Density Plot of DTW Distances",
    x = "DTW Distance",
    y = "Density"
  ) 
```

# Exporting the synchrony scores

In order to compare this to each pairs empathy score, we export as a csv to be used in another RMD

```{r}
write_csv(dtw_results_df, "data/dtw_results.csv")
```

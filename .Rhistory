# Extract and store principal components with dataframe name as key
pca_components_B <- pca_B$ind$coord[, 1:num_pca_components_to_use, drop = FALSE]
pca_components_list_B[[df_name_B]] <- pca_components_B
}
}
# Now you have two sets of lists:
# - pca_results_list_A and pca_results_list_B containing PCA results
# - pca_components_list_A and pca_components_list_B containing principal components
# with dataframe names as keys
rm(corr_matrix_A, corr_matrix_B, df, df_A, df_B, pca_A, pca_B)
## Setup chunk
knitr::opts_chunk$set(echo = TRUE, include = TRUE, message = FALSE, warning = FALSE)
pacman::p_load(
"XML",
"tidyverse",
"fs",
"assertthat",
"stringi",
"dtw",
"RTransferEntropy",
"signal",
"conflicted",
"Rcpp",
"future",
"dtw",
"dplyr",
"plotly",
"htmlwidgets",
"hrbrthemes",
"zoo"
)
conflicts_prefer(dplyr::filter)
## Making sure we are in the right directory
wd <- getwd()
if (basename(wd) != "PerceptionActionExam") {
setwd("./PerceptionActionExam")
}
##NADIA PATH##
# data_dir <- path_home() %>%
#   path("Documents", "GitHub", "PerceptionActionExam-Clean-up-attempt", "data", "tsvs") ## Nadia's path
#KATHARINA PATH##
data_dir <- path_home() %>%
path("Desktop","UNI", "3.semester", "Perception & Action", "PerceptionActionExam", "data", "tsvs") ## Katharina's path
#Aesthetic setup
theme_set(theme_ipsum(base_family = "Times New Roman"))
global_fill_colour <- "#8d5b5a"
aesthetic_palette <- c(
"#d8aeb5","#c17f8c","#b59592","#9b6f69","#a94f62","#8d5b5a","#684141","#733545","#523438","#48222b","#2f1a1b")
aesthetic_highlight_difference_palette <- c("#d8aeb5","#2f1a1b")
# Set the directory where your CSV files are located
data_folder <- "data/mocap_data_prepped"
# List all CSV files in the specified folder
csv_files <- list.files(data_folder, pattern = "*.csv", full.names = TRUE)
# Initialize an empty list to store the dataframes
list_of_dataframes <- list()
# Loop through each CSV file, read it into a dataframe, and store it in the list with its original file name
for (csv_file in csv_files) {
# Extract the file name without the extension
file_name <- tools::file_path_sans_ext(basename(csv_file))
# Read the CSV file into a dataframe and assign it to the list with the file name as the list name
list_of_dataframes[[file_name]] <- read.csv(csv_file)
}
list_of_dataframes_A <- list()
list_of_dataframes_B <- list()
# Get the names of the original dataframes
original_names <- names(list_of_dataframes)
for (i in seq_along(list_of_dataframes)) {
df <- list_of_dataframes[[i]]
df_name <- original_names[i]
# Split the dataframe based on 'subject' and omit rows with NAs
df_A <- na.omit(df[df$subject == 'A', ])
df_B <- na.omit(df[df$subject == 'B', ])
# Add the split and cleaned dataframes to their respective lists with modified names
if(nrow(df_A) > 0) {
list_of_dataframes_A[[paste(df_name, "A", sep = "_")]] <- df_A
}
if(nrow(df_B) > 0) {
list_of_dataframes_B[[paste(df_name, "B", sep = "_")]] <- df_B
}
}
rm(df, df_A, df_B, i, df_name)
# Define a function to replace NAs with column means
replace_nas_with_column_means <- function(df) {
df %>%
mutate(across(everything(), ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))
}
# Iterate over list_of_dataframes_A and replace NAs with column means
for (i in seq_along(list_of_dataframes_A)) {
list_of_dataframes_A[[i]] <- list_of_dataframes_A[[i]] %>%
ungroup() %>%
select(-condition, -index, -subject, -group) %>%
gather(key, value, -elapsed_time, -marker) %>%
unite(col = "new_col", marker, key, remove = TRUE) %>%
spread(key = new_col, value = value) %>%
arrange(elapsed_time) %>%
replace_nas_with_column_means()
}
# Iterate over list_of_dataframes_B and replace NAs with column means
for (i in seq_along(list_of_dataframes_B)) {
list_of_dataframes_B[[i]] <- list_of_dataframes_B[[i]] %>%
ungroup() %>%
select(-condition, -index, -subject, -group) %>%
gather(key, value, -elapsed_time, -marker) %>%
unite(col = "new_col", marker, key, remove = TRUE) %>%
spread(key = new_col, value = value) %>%
arrange(elapsed_time) %>%
replace_nas_with_column_means()
}
rm(i)
# Function to convert a dataframe to a matrix with standardized values for ICA
convert_df_to_matrix_for_ICA <- function(df) {
# Ensure the dataframe is sorted by elapsed_time
df_sorted <- df %>% arrange(elapsed_time)
# Select only numeric columns
df_numeric <- df_sorted %>% select(where(is.numeric))
# Standardize the data
df_standardized <- as.data.frame(scale(df_numeric))
# Return as matrix
return(as.matrix(df_standardized))
}
# Convert each dataframe in list_of_dataframes_A to a matrix for ICA
matrices_A <- lapply(list_of_dataframes_A, convert_df_to_matrix_for_ICA)
# Convert each dataframe in list_of_dataframes_B to a matrix for ICA
matrices_B <- lapply(list_of_dataframes_B, convert_df_to_matrix_for_ICA)
# Check for NAs in matrices_A
any(is.na(unlist(matrices_A)))
# Check for NAs in matrices_B
any(is.na(unlist(matrices_B)))
# Function to apply ICA on a matrix
apply_ica <- function(matrix_data, n_comp) {
# Assuming the first column is 'elapsed_time' and should not be included in ICA
matrix_for_ica <- matrix_data[, -1]  # Exclude 'elapsed_time'
# Run ICA (nAdiA:I CHANGED ROW.NORM)
if(ncol(matrix_for_ica) >= n_comp) {
ica_result <- fastICA(matrix_for_ica, n.comp = n_comp,alg.typ = "parallel", fun = "logcosh", alpha = 1,
method = "R", row.norm = TRUE, maxit = 200,
tol = 0.0001, verbose = TRUE)
return(ica_result)
} else {
warning(paste("Not enough columns in matrix to extract", n_comp, "components"))
return(NULL)
}
}
# Apply ICA on matrices in list_of_dataframes_A
ica_results_A <- lapply(matrices_A, apply_ica, n_comp = 1)  # Adjust n.comp as needed
# Apply ICA on matrices in list_of_dataframes_B
ica_results_B <- lapply(matrices_B, apply_ica, n_comp = 1)  # Adjust n.comp as needed
component_list_A <- lapply(ica_results_A, function(ica) ica$S)
component_list_B <- lapply(ica_results_B, function(ica) ica$S)
rm(ica_results_A, ica_results_B)
# Function to calculate the area between two components, aligning their lengths
calculate_area_between_components <- function(comp1, comp2) {
# Align lengths
min_length <- min(nrow(comp1), nrow(comp2))
comp1_aligned <- comp1[1:min_length, ]
comp2_aligned <- comp2[1:min_length, ]
# Calculate area
sum(abs(comp1_aligned - comp2_aligned))
}
# Function to align a component based on the smallest area between lines
align_component_based_on_area <- function(component_A, component_B) {
# Align component lengths for area calculation
original_area <- calculate_area_between_components(component_A, component_B)
flipped_area <- calculate_area_between_components(component_A, -component_B)
if (flipped_area < original_area) {
return(-component_B)
} else {
return(component_B)
}
}
# Align each pair of components from A and B
for (i in seq_along(component_list_A)) {
if (!is.null(component_list_A[[i]]) && !is.null(component_list_B[[i]])) {
component_list_B[[i]] <- align_component_based_on_area(component_list_A[[i]], component_list_B[[i]])
}
}
# Clean up
rm(list_of_dataframes_A, list_of_dataframes_B)
# Loop through each index of the component lists
for (i in seq_along(component_list_A)) {
# Extract components for Subject A and Subject B
component_A <- component_list_A[[i]]
component_B <- component_list_B[[i]]
# Ensure both components have the same number of rows
min_rows <- min(nrow(component_A), nrow(component_B))
# Combine both components into a single dataframe
combined_df <- data.frame(
time = rep(1:min_rows, 2),
value = c(component_A[1:min_rows, 1], component_B[1:min_rows, 1]),
group = rep(c("Subject A", "Subject B"), each = min_rows)
)
# Create a time series plot for both subjects
component_name_A <- names(component_list_A)[i]
component_name_B <- names(component_list_B)[i]
group_number <- paste(component_name_A, "vs", component_name_B)
p <- ggplot(combined_df, aes(x = time, y = value, color = group)) +
geom_line(size = 1) +
labs(
title = "Independent components",
subtitle = group_number,
x = "Time", y = "Component Value", color = "Subject") +
scale_color_manual(values = aesthetic_highlight_difference_palette)
# Print the plot
print(p)
}
library(fs)
# Create the "results/ICA" directory if it doesn't already exist
dir_create(path("results", "ICA"), recursive = TRUE)
# Function to save each matrix in the component list as a CSV
save_component_list_as_csv <- function(component_list, dir_path) {
# Ensure component_list has named indices
if (is.null(names(component_list))) {
stop("The component list must have named indices.")
}
for (comp_name in names(component_list)) {
# Use the component name for the file name
file_path <- file.path(dir_path, paste0(comp_name, ".csv"))
write.csv(component_list[[comp_name]], file_path, row.names = FALSE)
}
# Print a message to indicate completion
cat("All components have been saved as CSV in the '", dir_path, "' directory.\n")
}
# Example usage: Save component_list_A and component_list_B to "results/ICA" as CSV files
# Ensure that component_list_A and component_list_B have named indices
save_component_list_as_csv(component_list_A, "results/ICA")
save_component_list_as_csv(component_list_B, "results/ICA")
## Setup chunk
knitr::opts_chunk$set(echo = TRUE, include = TRUE, message = FALSE, warning = FALSE)
pacman::p_load(
"XML",
"tidyverse",
"fs",
"assertthat",
"stringi",
"dtw",
"RTransferEntropy",
"signal",
"conflicted",
"Rcpp",
"future",
"fastICA",
"groupICA",
"dtw",
"dplyr",
"plotly",
"htmlwidgets",
"hrbrthemes",
"zoo",
"corrplot",
"corrr",
"ggcorrplot",
"FactoMineR",
"factoextra"
)
conflicts_prefer(dplyr::filter)
## Making sure we are in the right directory
wd <- getwd()
if (basename(wd) != "PerceptionActionExam") {
setwd("./PerceptionActionExam")
}
##NADIA PATH##
# data_dir <- path_home() %>%
#   path("Documents", "GitHub", "PerceptionActionExam-Clean-up-attempt", "data", "tsvs") ## Nadia's path
#KATHARINA PATH##
data_dir <- path_home() %>%
path("Desktop","UNI", "3.semester", "Perception & Action", "PerceptionActionExam", "data", "tsvs") ## Katharina's path
#Aesthetic setup
theme_set(theme_ipsum(base_family = "Times New Roman"))
global_fill_colour <- "#8d5b5a"
aesthetic_palette <- c(
"#d8aeb5","#c17f8c","#b59592","#9b6f69","#a94f62","#8d5b5a","#684141","#733545","#523438","#48222b","#2f1a1b")
aesthetic_highlight_difference_palette <- c("#d8aeb5","#2f1a1b")
# Set the directory where your CSV files are located
data_folder <- "data/mocap_data_prepped"
# List all CSV files in the specified folder
csv_files <- list.files(data_folder, pattern = "*.csv", full.names = TRUE)
# Initialize an empty list to store the dataframes
list_of_dataframes <- list()
# Loop through each CSV file, read it into a dataframe, and store it in the list with its original file name
for (csv_file in csv_files) {
# Extract the file name without the extension
file_name <- tools::file_path_sans_ext(basename(csv_file))
# Read the CSV file into a dataframe and assign it to the list with the file name as the list name
list_of_dataframes[[file_name]] <- read.csv(csv_file)
}
convert_to_wide_format <- function(df) {
df %>%
unite("marker_subject", marker, subject, remove = TRUE) %>%
pivot_wider(
names_from = marker_subject,
values_from = c(x, y, z)
)
}
list_of_dataframes <- lapply(list_of_dataframes, convert_to_wide_format)
for (i in seq_along(list_of_dataframes)) {
list_of_dataframes[[i]] <- list_of_dataframes[[i]] %>%
select(-index, -condition, -group, -elapsed_time)
}
rm(i)
library(tidyr)
fill_NAs_with_LOCF_NOCB <- function(df) {
# Apply Last Observation Carried Forward (LOCF)
df_filled_locf <- apply(df, 2, function(x) {
if (all(is.na(x))) {
return(x)
} else {
return(na.locf(x, na.rm = FALSE))
}
})
# Convert back to dataframe
df_filled_locf <- as.data.frame(df_filled_locf)
# Apply Next Observation Carried Backward (NOCB)
df_filled_locf_nocb <- apply(df_filled_locf, 2, function(x) {
if (all(is.na(x))) {
return(x)
} else {
return(na.locf(x, fromLast = TRUE, na.rm = FALSE))
}
})
# Convert back to dataframe and return
return(as.data.frame(df_filled_locf_nocb))
}
list_of_dataframes <- lapply(list_of_dataframes, fill_NAs_with_LOCF_NOCB)
#quick na check
sapply(list_of_dataframes, function(df) any(is.na(df)))
standardize_and_normalize_dataframe <- function(df) {
# Standardize each column of the dataframe
standardized_df <- scale(df)
# Normalize the standardized dataframe using min-max scaling
min_max_normalized_df <- as.data.frame(apply(standardized_df, 2, function(x) {
(x - min(x)) / (max(x) - min(x))
}))
return(min_max_normalized_df)
}
list_of_dataframes <- lapply(list_of_dataframes, standardize_and_normalize_dataframe)
list_of_dataframes_A = list()
list_of_dataframes_B = list()
for (df_name in names(list_of_dataframes)) {
df = list_of_dataframes[[df_name]]
# Columns ending with _A
cols_A = grep("_A$", names(df), value = TRUE)
if (length(cols_A) > 0) {
list_of_dataframes_A[[paste0(df_name)]] = df[, cols_A]
}
# Columns ending with _B
cols_B = grep("_B$", names(df), value = TRUE)
if (length(cols_B) > 0) {
list_of_dataframes_B[[paste0(df_name)]] = df[, cols_B]
}
}
for (i in 1:16) {
# Ensure the index is within the list length
if (i <= length(list_of_dataframes_A)) {
corr_matrix_A <- cor(list_of_dataframes_A[[i]])
plot_A <- ggcorrplot(corr_matrix_A) +
ggtitle(paste("Correlation Matrix for DataFrame A", i))
print(plot_A)
}
if (i <= length(list_of_dataframes_B)) {
corr_matrix_B <- cor(list_of_dataframes_B[[i]])
plot_B <- ggcorrplot(corr_matrix_B) +
ggtitle(paste("Correlation Matrix for DataFrame B", i))
print(plot_B)
}
}
# Load the necessary library for PCA
library(FactoMineR)
# Create empty lists to store PCA results and principal components
pca_results_list_A <- list()
pca_results_list_B <- list()
pca_components_list_A <- list()
pca_components_list_B <- list()
# Define the number of principal components to use
num_pca_components_to_use <- 2  # You can adjust this as needed
for (i in 1:16) {
if (i <= length(list_of_dataframes_A)) {
# Extract the dataframe and its name
df_A <- list_of_dataframes_A[[i]]
df_name_A <- names(list_of_dataframes_A)[i]
# Perform PCA
pca_A <- PCA(df_A, graph = FALSE)
# Save PCA result in the list with dataframe name as key
pca_results_list_A[[df_name_A]] <- pca_A
# Extract and store principal components with dataframe name as key
pca_components_A <- pca_A$ind$coord[, 1:num_pca_components_to_use, drop = FALSE]
pca_components_list_A[[df_name_A]] <- pca_components_A
}
if (i <= length(list_of_dataframes_B)) {
# Extract the dataframe and its name
df_B <- list_of_dataframes_B[[i]]
df_name_B <- names(list_of_dataframes_B)[i]
# Perform PCA
pca_B <- PCA(df_B, graph = FALSE)
# Save PCA result in the list with dataframe name as key
pca_results_list_B[[df_name_B]] <- pca_B
# Extract and store principal components with dataframe name as key
pca_components_B <- pca_B$ind$coord[, 1:num_pca_components_to_use, drop = FALSE]
pca_components_list_B[[df_name_B]] <- pca_components_B
}
}
# Now you have two sets of lists:
# - pca_results_list_A and pca_results_list_B containing PCA results
# - pca_components_list_A and pca_components_list_B containing principal components
# with dataframe names as keys
rm(corr_matrix_A, corr_matrix_B, df, df_A, df_B, pca_A, pca_B)
library(factoextra)
library(hrbrthemes) # for theme_ipsum()
# Assuming global_fill_colour is defined
# Loop through each dataframe in list_of_dataframes
for (df_name in names(list_of_dataframes)) {
# Extract the current dataframe
g <- list_of_dataframes[[df_name]]
# Compute correlation matrix and perform PCA
corr_matrix <- cor(g)
data.pca <- princomp(corr_matrix)
# Create the scree plot
scree_plot <- fviz_eig(data.pca,
ncp = 5,
addlabels = TRUE,
barfill = global_fill_colour,
barcolor = global_fill_colour,
ggtheme = theme_ipsum()) +
ylim(0, 90) +
labs(subtitle = df_name) # Add dynamic subtitle
# Display or save the plot
print(scree_plot)
# To save: ggsave(paste0("ScreePlot_", df_name, ".png"), scree_plot)
}
# Function to calculate the area between two components, aligning their lengths
calculate_area_between_components <- function(comp1, comp2) {
# Align lengths
min_length <- min(nrow(comp1), nrow(comp2))
comp1_aligned <- comp1[1:min_length, ]
comp2_aligned <- comp2[1:min_length, ]
# Calculate area
sum(abs(comp1_aligned - comp2_aligned))
}
# Function to align a component based on the smallest area between lines
align_component_based_on_area <- function(component_A, component_B) {
# Align component lengths for area calculation
original_area <- calculate_area_between_components(component_A, component_B)
flipped_area <- calculate_area_between_components(component_A, -component_B)
if (flipped_area < original_area) {
return(-component_B)
} else {
return(component_B)
}
}
#THERE ARE ISSUERS WITH VERY LARGE VECTORS. IM TRYING TO COMBAT THAT PROBLEM HERE
batch_size <- 100  # Set a batch size that your system can handle
num_batches <- ceiling(length(pca_components_list_A) / batch_size)
for (batch in 1:num_batches) {
start_index <- (batch - 1) * batch_size + 1
end_index <- min(batch * batch_size, length(pca_components_list_A))
for (i in start_index:end_index) {
if (!is.null(pca_components_list_A[[i]]) && !is.null(pca_components_list_B[[i]])) {
pca_components_list_B[[i]] <- align_component_based_on_area(pca_components_list_A[[i]], pca_components_list_B[[i]])
}
}
# Optional: Save intermediate results to disk, clear memory, etc.
}
# Clean up
rm(list_of_dataframes_A, list_of_dataframes_B)
# Loop through each index of the component lists
for (i in seq_along(pca_components_list_A)) {
# Extract components for Subject A and Subject B
component_A <- pca_components_list_A[[i]]
component_B <- pca_components_list_B[[i]]
# Ensure both components have the same number of rows
min_rows <- min(nrow(component_A), nrow(component_B))
# Combine both components into a single dataframe
combined_df <- data.frame(
time = rep(1:min_rows, 2),
value = c(component_A[1:min_rows, 1], component_B[1:min_rows, 1]),
group = rep(c("Subject A", "Subject B"), each = min_rows)
)
# Create a time series plot for both subjects
component_name_A <- names(pca_components_list_A)[i]
component_name_B <- names(pca_components_list_B)[i]
group_number <- paste(component_name_A)
p <- ggplot(combined_df, aes(x = time, y = value, color = group)) +
geom_line(size = 1) +
labs(
title = "Principle components",
subtitle = group_number,
x = "Time", y = "Component Value", color = "Subject") +
scale_color_manual(values = aesthetic_highlight_difference_palette)
# Print the plot
print(p)
}
library(fs)
# Create the "results/PCA" directory if it doesn't already exist
dir_create(path("results", "PCA"), recursive = TRUE)
# Function to save each matrix in the component list as a CSV with an optional suffix
save_component_list_as_csv <- function(component_list, dir_path, suffix = "") {
# Ensure component_list has named indices
if (is.null(names(component_list))) {
stop("The component list must have named indices.")
}
for (comp_name in names(component_list)) {
# Use the component name for the file name, appending the suffix
file_path <- file.path(dir_path, paste0(comp_name, suffix, ".csv"))
write.csv(component_list[[comp_name]], file_path, row.names = FALSE)
}
# Print a message to indicate completion
cat("All components have been saved as CSV in the '", dir_path, "' directory.\n")
}
# Save pca_components_list_A and pca_components_list_B to "results/PCA" as CSV files
# The files from pca_components_list_A will have an '_A' suffix
save_component_list_as_csv(pca_components_list_A, "results/PCA", "_A")
# The files from pca_components_list_B can have no suffix or a different one (e.g., "_B")
save_component_list_as_csv(pca_components_list_B, "results/PCA", "_B")
